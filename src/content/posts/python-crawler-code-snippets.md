---
title: Python çˆ¬è™«å¸¸ç”¨ä»£ç ç‰‡æ®µ
published: 2024-03-06 23:40:00
draft: false
tags: [Python, Crawler]
category: Crawler
image: "./covers/13.jpg"
---

# python çˆ¬è™«

## ç”¨æˆ·è®¾ç½®

### ğŸ­è®¾ç½® HEADER

```python
import requests
header = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    } 
res = requests.get('https://www.douban.com/', headers=header) 
```

### ğŸš€è®¾ç½®ä»£ç† PROXY

```python
proxies = {
  'http': 'http://127.0.0.1:7890',
  'https': 'http://127.0.0.1:7890',
}
res = requests.get('https://www.douban.com/', proxies=proxies) 
```

## HTTP è¯·æ±‚

### â¡ï¸GET è¯·æ±‚é¡µé¢

```python
res = requests.get('https://www.jd.com/')
```

### â¬…ï¸POST è¯·æ±‚

```python
res = requests.post('http://www.xxxx.com', data={"key": "value"}) 
```

## è§£æå“åº”

### ğŸï¸äºŒè¿›åˆ¶æ•°æ®(eg.å›¾ç‰‡)

```python
res = requests.get(url)
with open('./cover.jpg', 'wb') as f:
    f.write(res.content)
```

### ğŸ“Šjson æ•°æ®

```python
import json
res = requests.get(url)
print(json.loads(res.text))
```

### ğŸ“‘HTML é¡µé¢

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(res.text,'html.parser')

print(soup.prettify())  # æŒ‰ç…§æ ‡å‡†çš„ç¼©è¿›æ ¼å¼è¾“å‡º
```

å…ƒç´ é€‰æ‹©

```python
elements = soup.select('#id') #é€‰æ‹©å™¨è¯­æ³•å’Œcssç±»ä¼¼ï¼Œè¿”å›list
```

å…ƒç´ å±æ€§

```python
element = elements[0]
print(element['name'])
print(element.get_text())
```

## ä¸€äº›çˆ¬è™«æŠ€æœ¯

### ğŸ˜´éšæœºç¡çœ 

```python
import time
import random

time.sleep(random.uniform(0,5))
```

### ğŸ”è¶…æ—¶é‡è¯•

```python
from requests.adapters import HTTPAdapter

s = requests.Session()
s.mount('http://', HTTPAdapter(max_retries=3))  # è¶…æ—¶é‡è¯•3æ¬¡
s.mount('https://', HTTPAdapter(max_retries=3))
res = s.get('http://www.xxxx.com', timeout=(10, 27))  # (connectè¶…æ—¶, readè¶…æ—¶)
```

### âšŸå¤šçº¿ç¨‹è¿è¡Œ

```python
import threading

# ä¿¡å·é‡ï¼Œç”¨æ¥é™åˆ¶çº¿ç¨‹æ•°
max_connections = 3
pool_sema = threading.BoundedSemaphore(max_connections)

url_list = [] # ç›®æ ‡urlåˆ—è¡¨
def craw_url(url): # ä¸»è¦åŠŸèƒ½å‡½æ•°
    pass
    pool_sema.release()

thread_list = []
for i in url_list:
    pool_sema.acquire()
    thread = threading.Thread(target=craw_url, args=(i,))
    thread.start()
    thread_list.append(thread)
for t in thread_list:
    t.join()
```

## å…¶ä»–åŠŸèƒ½å®ç°

### æ–°å»ºæ–‡ä»¶å¤¹

```python
import os

if not os.path.isdir(dir_path):
    os.mkdir(dir_path)
```

### æ—¥å¿—è¾“å‡º

å¼•å…¥ `logging` å `logging.info()` å³å¯ï¼Œä½†æ˜¯åœ¨ Jupyter notebook ä¸­éœ€è¦å…ˆè®¾ç½®ä¸€ä¸‹ï¼š

```python
import logging
logger = logging.getLogger()
 
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
 
# Setup file handler
fhandler  = logging.FileHandler('my.log') # æ—¥å¿—è¾“å‡ºåˆ° my.log
fhandler.setLevel(logging.DEBUG)
fhandler.setFormatter(formatter)
 
# Configure stream handler for the cells
chandler = logging.StreamHandler()
chandler.setLevel(logging.DEBUG)
chandler.setFormatter(formatter)
 
# Add both handlers
logger.addHandler(fhandler)
logger.addHandler(chandler)
logger.setLevel(logging.DEBUG)
 
# Show the handlers
logger.handlers

logger.info()
```

